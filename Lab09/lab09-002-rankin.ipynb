{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Document Analysis\n",
    "\n",
    "In this assignment, we will learn how to do document classification and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example\n",
    "\n",
    "In this example, we use [20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) dataset. Each sample is a document and there are totally 20 classes. \n",
    "\n",
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target labels: [7 4 4 ... 3 1 8]\n",
      "Train data target names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "#training samples: 11314\n",
      "#testing samples: 7532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(\"Train data target labels: {}\".format(data_train.target))\n",
    "print(\"Train data target names: {}\".format(data_train.target_names))\n",
    "\n",
    "print('#training samples: {}'.format(len(data_train.data)))\n",
    "print('#testing samples: {}'.format(len(data_test.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Represent documents with TF-IDF represention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631) (7532, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#TF-IDF representation for each document\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_train_vectors = vectorizer.fit_transform(data_train.data)\n",
    "data_test_vectors = vectorizer.transform(data_test.data) \n",
    "\n",
    "print(data_train_vectors.shape, data_test_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use KNN to do document classification\n",
    "\n",
    "Here, we use the cross-validation method to select $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "X_train = data_train_vectors\n",
    "y_train = data_train.target\n",
    "\n",
    "X_test = data_test_vectors\n",
    "y_test = data_test.target\n",
    "\n",
    "k_range = range(1, 5)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "clf_knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "grid = GridSearchCV(clf_knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use Logistic Regression to do document classification\n",
    "Here, we also use the cross-validation method to select the regularization coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "#=====training with cross validation======\n",
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid = GridSearchCV(clf_lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "#=====testing======\n",
    "clf_lr = LogisticRegression(penalty='l2', C=grid.best_params_['C'])\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(acc, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task: Document Classification and Clustering\n",
    "\n",
    "In this task, we are going to use [BBCNews](BBC_News_Train.csv) dataset. There are 1490 articles from 5 topics, including tech, business, sport, entertainment, politics. \n",
    "\n",
    "* Task 1: Please use KNN and logistic regression to do classification, and compare their performance.\n",
    "\n",
    "* Task 2: Please use K-means to partition this dataset into 5 clusters and find the representative words in each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data and represent it with TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target names: ['tech' 'entertainment' 'politics' 'business' 'sport']\n",
      "Training samples: 1266\n",
      "Testing samples: 224\n",
      "(1266, 22800) (224, 22800)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Reading the dataset\n",
    "df = pd.read_csv('BBC_News_Train.csv')\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.15)\n",
    "\n",
    "# Printing the unique target names in the training data\n",
    "print(\"Train data target names: {}\".format(df_train['Category'].unique()))\n",
    "\n",
    "# Printing the number of samples in the training and testing sets\n",
    "print('Training samples: {}'.format(len(df_train)))\n",
    "print('Testing samples: {}'.format(len(df_test)))\n",
    "\n",
    "# Creating a TfidfVectorizer object to convert text data to numerical vectors\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fitting the TfidfVectorizer object on the training data and transforming the training and testing data\n",
    "train_vectors = tfidf.fit_transform(df_train['Text'])\n",
    "test_vectors = tfidf.transform(df_test['Text']) \n",
    "\n",
    "# Printing the shape of the training and testing vectors\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use KNN to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257446080109553\n",
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "# Assign the training vectors and their corresponding labels to X_train and y_train\n",
    "X_train = train_vectors\n",
    "y_train = df_train['Category']\n",
    "\n",
    "# Assign the test vectors and their corresponding labels to X_test and y_test\n",
    "X_test = test_vectors\n",
    "y_test = df_test['Category']\n",
    "\n",
    "# Define a range of k values to test\n",
    "k_range = range(1, 5)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "# Create a KNeighborsClassifier object\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Create a GridSearchCV object to find the best hyperparameters\n",
    "grid = GridSearchCV(clf_knn, param_grid, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and hyperparameters\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use Logistic Regression to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 7}\n",
      "0.9821428571428571 0.9809102258540461 0.9821428571428571\n"
     ]
    }
   ],
   "source": [
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "    \n",
    "grid = GridSearchCV(clf_lr, param_grid, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2', C=grid.best_params_['C'])\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(acc, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Use K-means to do document clustering and find the 10 most representative words in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " mr\n",
      " labour\n",
      " election\n",
      " blair\n",
      " said\n",
      " party\n",
      " government\n",
      " brown\n",
      " howard\n",
      " minister\n",
      "Cluster 1:\n",
      " game\n",
      " england\n",
      " said\n",
      " cup\n",
      " win\n",
      " players\n",
      " team\n",
      " club\n",
      " chelsea\n",
      " match\n",
      "Cluster 2:\n",
      " film\n",
      " best\n",
      " awards\n",
      " award\n",
      " actor\n",
      " oscar\n",
      " won\n",
      " films\n",
      " director\n",
      " comedy\n",
      "Cluster 3:\n",
      " mobile\n",
      " people\n",
      " said\n",
      " software\n",
      " microsoft\n",
      " users\n",
      " technology\n",
      " digital\n",
      " phone\n",
      " music\n",
      "Cluster 4:\n",
      " said\n",
      " year\n",
      " sales\n",
      " growth\n",
      " mr\n",
      " company\n",
      " economy\n",
      " bank\n",
      " market\n",
      " oil\n"
     ]
    }
   ],
   "source": [
    "# Define the number of clusters to use for KMeans clustering\n",
    "clf_kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "# Fit the KMeans model to the training data\n",
    "y = clf_kmeans.fit(X_train)\n",
    "\n",
    "# Get the indices that would sort the cluster centers in descending order for each cluster\n",
    "order_centroids = clf_kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Get the list of feature names (terms) used in the TfidfVectorizer\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "# For each cluster, print the top 10 terms with the highest tf-idf scores\n",
    "for i in range(5):\n",
    "     print(\"Cluster %d:\" % i)\n",
    "     for ind in order_centroids[i, :10]:\n",
    "         print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
